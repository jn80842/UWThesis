\chapter{Introduction}
\label{sec:intro}

Halide is a widely used, open source image processing language intended to deliver very high performance in its compiled programs. Like many compilers, in order to apply optimizations, the Halide compiler must perform a variety of analyses of
  the input program's properties. For example, if the user marks a
  loop to be fully unrolled, the compiler must infer a constant upper
  bound for the extent of the loop. If the user marks
  a loop as parallel, the compiler must prove the absence of data
  races. These analyses also affect performance more than in most
  compilers. In Halide, the compiler infers loop bounds and allocation sizes.
  If these are overestimated, the generated code may
  perform an amount of wasted work sufficient to alter the
  computational complexity of the algorithm. The reasoning engines that make these judgments within the Halide compiler are crucial to exploiting opportunities for these optimizations.
  In fact, Halide relies so heavily on one of these engines that restricting it
  to mere constant-folding causes a geomean 5.1$\times$ increase in
  compilation times and a 26.4$\times$ increase in runtimes across
  Halide's benchmark suite.

\modified{Such a reasoning engine must balance three key criteria:}

\begin{itemize}
  \item \modified{\textbf{Completeness:}
It must work in 
the theory of integers, which is undecidable, and make use of operations such as 
Euclidean division and maximum/minimum which can be especially difficult for 
automated reasoning. Any solver in this theory is necessarily incomplete, but in general, 
the compiler can generate higher-performing code as the reasoning engine becomes more powerful.}
  \item \modified{\textbf{High performance:} However, the engine is called many thousands of 
times over the course of a single compilation, and so requires high performance and low memory usage. }
  \item \modified{\textbf{Determinism:} The compiler must always return the same 
result for the same program, regardless of what platform runs the compiler, 
so the engine must be deterministic.}
\end{itemize}

Halide addresses these reasoning tasks with \emph{term rewriting systems}. Using a custom algorithm 
that greedily applies rules in a fixed order and keeps only the expression being currently 
rewritten as state, a term rewriting system (TRS) can provide the performance and determinism 
that the compiler requires. This algorithm scales well in terms of the number of rules
in the TRS, so Halide developers continue to improve and increase the reasoning engines'
power by refining the ruleset and adding new rules by hand.

However, maintaining term rewriting systems by hand presents significant challenges. Rules are 
carefully inspected and fuzz-tested, but are not formally proven sound. The 
rule application algorithm itself is not guaranteed to terminate, so changing, adding, or 
reordering rules may result in cycles on untested inputs. Debugging the system can be 
difficult, since it is not clear which rule or combination of rules is responsible for 
undesired behavior. The TRS is not complete enough to address all compiler queries 
(for example, it may fail to compute tight bounds on intermediate arrays, leading to over-allocations).
Finally, the precise goal of a reasoning engine, or the strategy it uses to achieve that goal,
may be recorded only in the form of the rewrite rules themselves. If developers are not familiar with 
an entire ruleset, or the full variety of applications in which the ruleset is invoked, they may inadvertently 
add a rule that makes the simplifier worse for some of its usecases.

In this work, we propose to show how authoring and maintaining term rewriting systems can be made easier through the use of formal methods and program synthesis. We will demonstrate how these techniques can support all degrees of human involvement: from verifying the correctness of an existing handwritten term rewriting system, to learning new rules to augment an existing system, to synthesizing a new ruleset entirely from scratch.

In this work we make the following contributions:

\begin{itemize}
    \item \emph{We show formal methods and program synthesis can usefully assist the authors of term rewriting systems.} Formal proofs of soundness and guarantees of termination can find bugs in large, widely-deployed term rewriting systems, and automatic program synthesis can be used to find new rewrite rules to augment these systems. Our work has identified and patched bugs in the Halide codebase, been used by Halide developers to support code maintenance, and automatically produced new rewrite rules that have been merged into the compiler.
    \item \emph{We present an effective means of encoding a specification for the synthesis of term rewriting systems.} Besides providing a proof of termination, we claim that reduction orders can serve as a useful formalization of the intent behind a term rewriting system, and can be used as a specification when synthesizing rewrite rules.
\end{itemize}

We evaluate our work in two case studies. In the first, we investigate the first claim by using formal methods to enhance a pivotal term rewriting system within the Halide compiler called the \emph{simplifier}. We found that we were able to identify bugs, prove the absence of future errors, and increase the rewriting power of the system. We review our evaluation of the first claim in section~\ref{sec:prior}. 

In the second case study, we take a look at a smaller and less mature term rewriting system in Halide called the \emph{variable solver}. Previously, we synthesized new rules for the simplifier largely guided by its large existing ruleset. Here, we plan to evaluate our proposed means of writing specifications for term rewriting system by synthesizing a ruleset entirely from scratch. We lay out our means of writing specification and our evaluation of the synthesized rulesets in chapter~\ref{chapter:synthfromscratch}.

\section{Maintaining an existing term rewriting system}
\label{sec:prior}

The Halide compiler contains a hand-authored term rewriting system commonly called the simplifier. It is used in many cases within the compiler for making expressions shorter and in a form better suited to downstream uses. Sometimes the simplifier is used as a prover: the truth value of an expression is checked by rewriting it with the simplifier to see if it will be rewritten to the constant \htrue. At the time we began this work, the simplifier was fairly mature: it had been deployed in production for over a year, was both unit tested and fuzz tested, and comprised almost a thousand rules.

Is it necessary to apply formal methods to the authoring of term rewriting systems? In chapter~\ref{chapter:verification}, we show that applying formal methods can identify and remedy real issues in the simplifier TRS. Conversely, we observe that implementing expression-transforming code as a term rewriting system is useful precisely because it allows easy integration with formal methods. We also show in chapter~\ref{chapter:synthesis} that we can improve this term rewriting system by synthesizing new rewrite rules automatically and that this synthesis process can be used in the compiler's development process. This work has been published at OOPSLA 2020~\citep{newcomb2020verifying}.

\subsection{Proof of soundness}
First, we formally verify that each rule in the Halide simplifier ruleset is semantics-preserving, demonstrating that proofs of soundness are possible despite the lack of a 
decision procedure for our theory. We do this by modeling the semantics of the Halide expression language in SMT2. We then implemented a pretty-printer to translate each rule in the simplifier ruleset to an SMT query to be checked by the solver Z3~\citep{de2008z3}. About 12\% of the ruleset could not be verified by Z3, and we proved those rules correct by hand using the proof assistant Coq~\citep{Coq19}. Even though the code had been deployed for over a year and had been fuzz-tested, we found that four of the existing rules were incorrect and submitted patches. In constructing the Coq proofs, we also noticed that 17 rules had predicates that were overly conservative, and submitted patches for the relaxed predicates as well.

While this project was ongoing, the Halide semantics for division was changed: division by zero was no longer undefined behavior, but now returned zero. It was simple for us to amend our modeled semantics and rerun rule verification in Z3. Again about 12\% of the ruleset had to be hand-proven using Coq, but we were able to leverage many of the existing proofs from our previous verification. Our verification identified 44 rules which were not correct under the new semantics and 37 rules whose predicates could be relaxed under the semantics change. This shows the value of our formal methods infrastructure, which allowed Halide developers to push a fairly major change with a higher degree of confidence in the soundness of the simplifier than could have been achieved with either manual testing or fuzzing.

\subsection{Proof of termination}
The Halide simplifier algorithm successively applies rewrite rules until the resulting expression can no longer be rewritten. Thus, if some sequence of rewrites to some input expression can form a cycle, the simplifier algorithm will not terminate. These non-termination errors have been observed in the past (resulting in the compiler throwing a stack overflow error and crashing). Without a specific input expression on which to reproduce a cycle, it is very difficult to examine a set of around a thousand rules and find a subset on which a cycle could occur; even once a cycle has been identified, it is difficult to know the best way to repair it. Without insight into how these cycles might be formed, it was possible to introduce The ruleset was thus very brittle; deleting, altering, or reordering existing rules has caused new non-termination errors in the past as well.

A term rewriting system can be proven to terminate using a formalism called a \emph{reduction order}~\citep{baader1999term}. A reduction order is an order over a language of terms; if for every rule in a term rewriting system, we can show that the rule's left-hand side is strictly greater than the right-hand side in this order, then we know that there can be no set of rules that can form a cycle, and thus the ruleset must always terminate. A reduction order is distinguished by a few special properties to ensure that these ordering holds no matter how input expressions are matched to the left-hand side term, as well as when a rule is used to rewrite a subterm inside of a larger input expression.

We devised a reduction order that fit as many of the existing simplifier rules as possible. Eight rules could not be fit to our ordering, and we submitted patches to either delete or modify them. Once this was done, not only was a class of bugs eliminated, but the ruleset could now be safely modified without fear of introducing new non-termination behavior. It was also now safe to add any new rule so long as the rule conformed to the reduction order. 

We observe that this reduction order serves to encode the meaning of simplification in the context of the Halide TRS
by formalizing what it means for a rule to usefully modify an expression.  While
many notions of ``simpler'' are possible, we encode the specific criteria for Halide
expressions by defining an \emph{ordering} over the left-hand and right-hand terms of 
the rules in the TRS that captures the intentions behind the rewrites. This ordering 
means that every local change caused by the application of a rewrite rule moves the 
expression in some useful direction. By composing several orders lexicographically, 
we can express many different intuitions about what makes an expression simpler in a
defined priority: we may want to remove as many vector operations as possible, 
then reduce the overall size of the expression, and so on. Maintaining this order as 
an invariant over any future rules means any additions to the ruleset will not undo 
progress made by existing rules.

\subsection{Strengthening the ruleset}
We can empirically observe that the simplifier ruleset is not sufficiently powerful to deal with all expressions it may be called upon, by instrumenting the compiler and logging any expressions that the simplifier cannot further solve. In the past, ruleset authors might look at these failed expressions and write rules to address them by hand. In chapter~\ref{chapter:synthesis}, we automated this progress by synthesizing these rules instead.

However, even
given the constraints imposed by the termination order, the space of equivalences 
in the Halide expression language is infinitely large. How do we choose which  
rules to add? We observe that there is some bias on the distribution of expressions 
seen by the compiler on realistic inputs over the full expression space. We take advantage 
of this bias by gathering expressions from realistic compilations on which the current 
TRS is ``stuck'' and can make no further progress. We synthesize rules through a pipeline that takes these expression as input and uses CEGIS loops to synthesize an equivalent right-hand side and (if necessary) a predicate guard that indicates when it is safe to apply 
the rule. Our pipeline produces more general rules by mining input expressions for larger patterns and by replacing constants with fresh variables. We also find variants on synthesized rules by applying associativity and commutitativity laws to their left-hand sides. We choose candidate left-hand sides 
for rules from this corpus and synthesize equivalent right-hand terms that obey the 
termination order. In our experiments, we found that our synthesis pipeline could produce patches to ruleset bugs as good or better than those that were authored by hand. We also observed Halide developers integrating the rule synthesizer into their workflow. Our synthesis procedure is also capable of finding large numbers of useful rules without
human oversight; although the existing compiler is mature 
and well-tuned for our suite of benchmarks, we show some performance 
gains without increases in compilation time when our newly synthesized suite 
of rules is added to the TRS. 

\section{Synthesizing a new term rewriting system}
\label{intro:varsolver}

For the Halide simplifier, we synthesized individual rules to augment an already mature term rewriting system. Next, we demonstrate the synthesis of term rewriting systems completely from scratch. We do this using the Halide \emph{variable solver} as a case study. The variable solver takes a Halide expression and a variable found within that expression (the \emph{target variable}) and attempts to eliminate as many instances of that variable as it can and isolate the variable to one side of the expression as best it can. (If the expression is an equality, this would consitute `solving' the expression for the chosen variable.) 

An idealized version of our proposed synthesis procedure is laid out in algorithm~\ref{algo:synthesis}. This procedure requires a goal function, which returns true if a term is in some desired form and false otherwise; a semantic equivalence relation; and a reduction order over terms, as well of a set of training expressions $E$, which should be representative of the types of expressions the term rewriting system will take as input and a set of test expressions $S$ on which to evaluate the behavior of the term rewriting system.

The key input to the synthesis procedure is the reduction order over terms, which serves as a specification for the synthesis of rules. The reduction order is a means of formalizing either the intent of the term rewriting system\textemdash the form into which the TRS should rewrite expressions\textemdash or the strategy by which the TRS will rewrite expressions to be closer to the desired form. While TRS authors are likely to have good intuition as to what rewriting strategy their system should employ, choosing the best reduction order for the task is still not a trivial exercise. Our synthesis process gives users the chance to experiment and determine which reduction order serves their purpose best; rather than hand-turning rules, they can synthesize a ruleset entirely from scratch for a given reduction order and evaluate it directly.

We have used the Halide variable solver as a case study to demonstrate how synthesis can be used to evaluate reduction order specifications. We lay out the refined synthesis algorithm, with some discussion of how best to search the space of candidate rules, and show how reduction orders can be encoded efficiently as metasketches. We evaluate our claims by synthesizing two rulesets from two different reduction orders in a fully automated way and evaluate their performance against the existing handwritten solver on a suite of benchmarks.

\section{Outline of work}

In this dissertation, we first lay out some necessary background in chapter~\ref{chapter:overview}: we give a formal definition of term rewriting systems and related concepts, describe the two TRSs within the Halide compiler that will serve as our case studies, and provide some discussion of the rationale behind some of the Halide rewriting algorithm design choices. In chapter~\ref{chapter:verification}, we detail our work proving the correctness of the Halide simplifier, showing that the TRS is both semantics-preserving and guaranteed to terminate. Next, in chapter~\ref{chapter:synthesis}, we show how we were able to increase the robustness of the simplifier TRS by using a key artifact from our termination proof, a reduction order, as a specification to synthesize new rewrite rules. We demonstrate that our synthesis process can produce high-quality bug fixes, discuss some ways in which our techniques have been used to support Halide developers' work, and perform a large-scale experiment synthesizing an additional 4,000 new rules, with the effect of reducing peak memory usage of benchmark programs with no increase in compilation times. In chapter~\ref{chapter:synthfromscratch}, we synthesize a ruleset for the Halide variable solver entirely from scratch, using our method of specifying rulesets through reduction orders. We discuss the efficient encoding of reduction order specifications using sketches, and furthermore show that our synthesis pipeline gives users an automatic tool for empirically evaluating alternative specifications without ever having to manually write rules. Finally, in chapter~\ref{chapter:conclusion}, we conclude with some directions for future work.
