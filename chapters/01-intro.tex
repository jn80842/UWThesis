\chapter{Introduction}
\label{sec:intro}

Term rewriting systems are a useful tool in the construction of compilers. They are a succinct and efficient way of executing program reasoning tasks, and can give extremely fast performance. 

However, term rewriting systems can be challenging both to develop and to maintain. As the path of execution for inputs can vary greatly, it can be difficult for developers to reason about how the various rules in the system interact as a whole. Adding, altering, or deleting a rule can have unforeseen consequences. Even if a TRS contains unsound rules, it may give correct results on all but a few inputs, and when the error is detected it can be difficult to track down which rules is responsible. A slight change to the ruleset can make it possible for rules form cycles, causing the system to fail to terminate on some inputs. 

The task of authoring these systems is even more challenging when the language of input expressions is in an undecidable theory. Since a term rewriting system in such a language cannot be complete, it is hard to know if a given system is optimal or if some change would improve its performance, degrade it, or perhaps make no difference at all.

In this work, we propose to show how authoring and maintaining term rewriting systems can be made easier through the use of formal methods and program synthesis. We will demonstrate how these techniques can support all degrees of human involvement: from verifying and strengthening existing term rewriting systems, to bootstrapping term rewriting systems from a specification and a set of seed axioms, to synthesizing a new ruleset entirely from scratch.

In this work we make the following contributions:

\begin{itemize}
    \item \emph{We show formal methods and program synthesis can usefully assist the authors of term rewriting systems.} Formal proofs of soundness and guarantees of termination can find bugs in large, widely-deployed term rewriting systems, and automatic program synthesis can be used to find new rewrite rules to augment these systems.
    \item \emph{We present an effective means of encoding a specification for the synthesis of term rewriting systems.} Besides providing a proof of termination, we claim that reduction orders can serve as a useful formalization of the intent behind a term rewriting system, and can be used as a specification when synthesizing rewrite rules.
\end{itemize}

We evaluate our work in two case studies. In prior work, we investigated the first claim by using formal methods to enhance a pivotal term rewriting system within the Halide compiler called the simplifier. We found that we were able to identify bugs, prove the absence of future errors, and increase the rewriting power of the system. We review our evaluation of the first claim in section~\ref{sec:prior}. 

In our proposed work, we take a look at a smaller and less mature term rewriting system in Halide called the variable solver. In prior work we synthesized new rules for the simplifier largely guided by its large existing ruleset. Here, we plan to evaluate our proposed means of writing specifications for term rewriting system by synthesizing rules to add to the existing ruleset and by synthesizing a ruleset entirely from scratch. We lay out our proposed means of writing specification and our plan to evaluate it in section~\ref{proposed}.

We survey related work in section~\ref{sec:related} and conclude in section~\ref{sec:conclusion}.

\section{Prior work: Maintaining an existing term rewriting system}
\label{sec:prior}

The Halide compiler contains a hand-authored term rewriting system commonly called the simplifier. It is used in many cases within the compiler for making expressions shorter and in a form better suited to downstream uses. Sometimes the simplifier is used as a prover: the truth value of an expression is checked by rewriting it with the simplifier to see if it will be rewritten to the constant true. At the time we began this work, the simplifier was fairly mature: it had been deployed in production for over a year, was both unit tested and fuzz tested, and comprised almost a thousand rules.

Is it necessary to apply formal methods to the authoring of term rewriting systems? In previous work, we showed that applying formal methods can identify and remedy real issues in the simplifier TRS. Conversely, implementing expression-transforming code as a term rewriting system is useful precisely because it allows easy integration with formal methods. We also show that we can improve this term rewriting system by synthesizing new rewrite rules automatically and that this synthesis process can be used in the compiler's development process. We first provide a proof of soundness and of termination, and then detail the rule synthesis process below. This work has been published at OOPSLA 2020~\cite{newcomb2020verifying}.

\subsection{Proof of soundness}
We first verify that each rule in the Halide simplifier ruleset is semantics-preserving. We do this by modeling the semantics of the Halide expression language in SMT2. We then implemented a pretty-printer to translate each rule in the simplifier ruleset to an SMT query to be checked by the solver Z3~\cite{de2008z3}. About 12\% of the ruleset could not be verified by Z3, and we proved those rules correct by hand using the proof assistant Coq~\cite{Coq19}. Even though the code had been deployed for over a year and had been fuzz-tested, we found that four of the existing rules were incorrect and submitted patches. In constructing the Coq proofs, we also noticed that 17 rules that predicates that were overly conservative and submitted patches for the relaxed predicates as well.

While this project was ongoing, the Halide semantics for division was changed: division by zero was no longer undefined behavior, but now returned zero. It was simple for us to amend our modeled semantics and rerun rule verification in Z3. Again about 12\% of the ruleset had to be hand-proven using Coq, but we were able to leverage many of the existing proofs from our previous verification. Our verification identified 44 rules which were not correct under the new semantics and 37 rules whose predicates could be relaxed under the semantics change. This shows the value of our formal methods infrastructure, which allowed Halide developers to push a fairly major change with a higher degree of confidence in the soundness of the simplifier than could have been achieved with either manual testing or fuzzing.

\subsection{Proof of termination}
The Halide simplifier algorithm successively applies rewrite rules until the resulting expression can no longer be rewritten. Thus, if some sequence of rewrites to some input expression can form a cycle, the simplifier algorithm will not terminate. These non-termination errors have been observed in the past (resulting in the compiler throwing a stack overflow error and crashing). Without a specific input expression on which to reproduce a cycle, it is very difficult to examine a set of around a thousand rules and find a subset on which a cycle could occur; even once a cycle has been identified, it is difficult to know the best way to repair it. The ruleset was thus very brittle; deleting, altering, or reordering existing rules has caused new non-termination errors in the past as well.

A term rewriting system can be proven to terminate using a formalism called a \emph{reduction order}~\cite{baader1999term}. A reduction order is an order over a language of terms; if for every rule in a term rewriting system, we can show that the rule's left-hand side is strictly greater than the right-hand side in this order, then we know that there can be no set of rules that can form a cycle, and thus the ruleset must always terminate. A reduction order is distinguished by a few special properties to ensure that these ordering holds no matter how input expressions are matched to the left-hand side term, as well as when a rule is used to rewrite a subterm inside of a larger input expression; see ~\cite{newcomb2020verifying} for full details.

We devised a reduction order that fit as many of the existing simplifier rules as possible. Eight rules could not be fit to our ordering, and we submitted patches to either delete or modify them. Once this was done, not only was a class of bugs eliminated, but the ruleset could now be safely modified without fear of introducing new non-termination behavior. It was also now safe to add any new rule so long as the rule conformed to the reduction order. 

\subsection{Strengthening the ruleset}
We can empirically observe that the simplifier ruleset is not sufficiently powerful to deal with all expressions it may be called upon, by instrumenting the compiler and logging any expressions that the simplifier cannot further solve. In the past, ruleset authors might look at these failed expressions and write rules to address them by hand. In ~\cite{newcomb2020verifying}, we automated this progress by synthesizing these rules instead.

We synthesize rules through a pipeline that took an expression as input and used CEGIS loops to synthesize an equivalent right-hand side and (if necessary) a predicate guard to form a rule. Our pipeline produces more general rules by mining input expressions for larger patterns and by replacing constants with fresh variables. We also find variants on synthesized rules by applying associativity and commutitativity laws to their left-hand sides. In our experiments, we found that our synthesis pipeline could produce patches to ruleset bugs as good or better than those that were authored by hand. We also carried out a large-scale experiment that synthesized over 4,000 rules using input expressions gathered from compilations of realistic benchmarks; when these new rules were added to the simplifier, the enhanced compiler produced programs with reduced peak memory usage, while showing no increase in compilation times. We also observed Halide developers integrating the rule synthesizer into their workflow.

\section{Proposed work: Synthesizing a new term rewriting system}
\label{proposed}

In our prior work, we synthesized individual rules to augment an already mature term rewriting system. In our proposed work, we plan to synthesize term rewriting systems from a small existing ruleset or completely from scratch. Our pipeline is sketched out below. In this section, we first justify the assumptions behind our approach. We then lay out our planned evaluation of our methodology on a new application, the variable solver term rewriting system within the Halide compiler.

An idealized version of our proposed synthesis procedure is laid out in algorithm~\ref{algo:synthesis}. This procedure requires a goal function, which returns true if a term is in some desired form and false otherwise; a semantic equivalence relation; and a reduction order over terms, as well of a set of training expressions $E$, which should be representative of the types of expressions the term rewriting system will take as input and a set of test expressions $S$ on which to evaluate the behavior of the term rewriting system.

\section{OOPSLA introduction}
\jln{it's longer than the generals intro so make sure all relevant content is integrated}
\modified{To compile an image processing pipeline written in the
  Halide language, the compiler must perform a variety of analyses of
  the pipeline's properties. For example, if the user marks a
  loop to be fully unrolled, the compiler must infer a constant upper
  bound for the extent of the loop. If the user marks
  a loop as parallel, the compiler must prove the absence of data
  races. These analyses also affect performance more than in most
  compilers. In Halide, the compiler infers loop bounds and allocation sizes.
  If these are overestimated, the generated code may
  perform an amount of wasted work sufficient to alter the
  computational complexity of the algorithm. These analyses all depend
  critically on the quality of Halide's expression simplifier. In
  fact, Halide relies so heavily on its simplifier that restricting it
  to mere constant-folding causes a geomean 5.1$\times$ increase in
  compilation times and a 26.4$\times$ increase in runtimes across
  Halide's benchmark suite.  }

%\modified{When the Halide language compiles an image processing pipeline, its compiler must issue a 
%variety of queries over the pipeline's properties. For example, it may attempt to prove a 
%loop extent can be reduced to a constant, allowing the loop to be fully unrolled; it may 
%check properties as part of dependency analysis to show that two loops contain no hazards 
%and can be parallelized; or it may try to prove that certain optimizations can be safely 
%performed, such as lowering code to cheaper instructions. The solver that addresses these 
%queries is so vital to the compiler that removing it results in 5.1$\times$ increase in compilation 
%times and 26.4$\times$ increase in runtimes for even simple benchmarks. }

\modified{This simplifier must balance three key criteria:}

\begin{itemize}
  \item \modified{\textbf{Completeness:}
It must work in 
the theory of integers, which is undecidable, and make use of operations such as 
Euclidean division and maximum/minimum which can be especially difficult for 
automated reasoning. Although any solver in this theory is necessarily incomplete, but in general, 
the compiler can generate higher-performing code as the simplifier becomes more powerful.}
  \item \modified{\textbf{High performance:} However, the simplifier is called many thousands of 
times over the course of a single compilation, and so requires high performance and low memory usage. }
  \item \modified{\textbf{Determinism:} The compiler must always return the same 
result for the same program, regardless of what platform runs the compiler, 
so the simplifier must be deterministic. This is a hard requirement.}
\end{itemize}

%\modified{Determinism is a hard requirement for the design of the
%  simplifier, which rules out techniques with unbounded runtimes that
%  rely on timeouts.  Given that constraint, the Halide simplifier must
%  address as many of the compiler's queries as possible while
%  maintaining high performance.}

\modified{Halide addresses this problem with a \emph{term rewriting system}. Using a custom algorithm 
that greedily applies rules in a fixed order and keeps only the expression being currently 
rewritten as state, the term rewriting system (TRS) provides the performance and determinism 
that the compiler requires. This algorithm scales well in terms of the number of rules
in the TRS, so Halide developers continue to improve and increase its
power by refining the ruleset and adding new rules by hand.}

\modified{However, maintaining a TRS by hand presents significant challenges. Rules are 
carefully inspected and fuzz-tested, but are not formally proven sound. The 
rule application algorithm itself is not guaranteed to terminate, so changing, adding, or 
reordering rules may result in cycles on untested inputs. Debugging the system can be 
difficult, since it is not clear which rule or combination of rules is responsible for 
undesired behavior. The TRS is not complete enough to address all compiler queries 
(for example, it may fail to compute tight bounds on intermediate arrays, leading to over-allocations).
Finally, although the overarching goal of the TRS is to simplify 
expressions, ``simplify'' is an imprecise notion. If developers are not familiar with 
the entire ruleset, or the full variety of applications of the simplifier, they may inadvertently 
add a rule that makes the simplifier worse for some of its usecases. }

\modified{ In this work, we show how techniques based on formal methods can help developers
 maintain a complex term rewriting system, first by providing formal guarantees 
 of soundness and termination, then by growing the ruleset to increase the simplifier's 
 power while ensuring the performance and determinism required by a compiler in 
 industrial use.  We further demonstrate a strategy for growing a simplifier's power
 when completeness is impossible, by synthesizing new rules that operate in
 the subset of the theory encountered during real-world compilation.}

\modified{First, we formally verify the existing 
ruleset, demonstrating that proofs of soundness are possible despite the lack of a 
decision procedure for our theory. We model the Halide expression language and verify 
the ruleset via an SMT solver that can prove about 88\% of the existing ruleset;
we prove the remaining rules by hand via the proof assistant Coq~\cite{Coq19}. We find four unsound rules, 
as well as several rules which could be made more general by relaxing their predicate guards. 
The Halide developers changed the language's semantics for division during this work; we reran our verification 
process to find 44 rules which were incorrect under the new semantics, demonstrating 
the usefulness of our technique.}

\modified{Next, we define the meaning of simplification in the context of the Halide TRS
by formalizing what it means for a rule to usefully modify an expression.  While
many notions of ``simpler'' are possible, we encode the specific criteria for Halide
expressions by defining an \emph{ordering} over the left-hand and right-hand terms of 
the rules in the TRS that captures the intentions behind the rewrites. This ordering 
means that every local change caused by the application of a rewrite rule moves the 
expression in some useful direction. By composing several orders lexicographically, 
we can express many different intuitions about what makes an expression simpler in a
defined priority: we may want to remove as many vector operations as possible, 
then reduce the overall size of the expression, and so on. Usefully, this order also provides a termination guarantee: 
if every rewrite leaves the rewritten expression strictly less in terms of our order, 
then it is impossible for any sequence of rewrites to form cycles, so the rewrite 
algorithm must terminate. This is not a hypothetical issue: Halide developers have previously
observed non-termination when adding rules.  We devise an ordering that fits as many of the existing rules as 
possible, and then remove the few rules that do not obey this ordering, proving
that the TRS will now always terminate. Maintaining this order as 
an invariant over any future rules means any additions to the ruleset will not undo 
progress made by existing rules; it also means rules can be freely removed or reordered 
without affecting termination.}

\modified{Having guaranteed soundness and termination, we increase 
the solving power of the TRS. We do this through \emph{synthesis} of new rules. Even
given the constraints imposed by the termination order, the space of equivalences 
in the Halide expression language is infinitely large. How do we choose which  
rules to add? We observe that there is some bias on the distribution of expressions 
seen by the compiler on realistic inputs over the full expression space. We take advantage 
of this bias by gathering expressions from realistic compilations on which the current 
TRS is ``stuck'' and can make no further progress. We choose candidate left-hand sides 
for rules from this corpus and synthesize equivalent right-hand terms that obey the 
termination order. \modifiedagain{These expressions frequently contain constants, so we
generalize rules by replacing constants
with fresh variables and synthesizing predicate guards that indicate when it is safe to apply 
the rule.} Our synthesis procedure finds large numbers of useful rules without
human oversight; although the existing compiler is mature 
and well-tuned for our suite of benchmarks, we show some performance 
gains without increases in compilation time when our newly synthesized suite 
of rules is added to the TRS. }


\modified{In the rest of the paper, we first provide some background on term rewriting systems 
and the Halide rewriting algorithm (Section~\ref{sec:term-rewriting-in-halide}).
We then describe our approach to verifying
rule correctness (Section~\ref{sec:verification}) and discuss the relation between LHS and RHS terms that guarantees 
termination (Section~\ref{sec:termination}). Once semantics and the termination property have been formalized, 
we present our synthesis algorithm (Section~\ref{sec:completeness}). We demonstrate the effectiveness of our approach 
through several experiments and case studies (Section~\ref{sec:evaluation}), with observations on the ways Halide developers 
have been able to integrate our techniques into their workflows. Finally, we close in Sections~\ref{sec:limitations}--\ref{sec:conclusion}
 with a discussion of our current limitations and a review of related work.}